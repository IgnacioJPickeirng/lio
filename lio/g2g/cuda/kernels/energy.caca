template<class scalar_type, bool compute_energy, bool compute_factor, bool lda>
__global__ void gpu_compute_density(scalar_type* const energy, scalar_type* const factor, const scalar_type* const point_weights,
                                    uint points, const scalar_type* rdm, const scalar_type* function_values, const vec_type<scalar_type,4>* gradient_values,
                                    const vec_type<scalar_type,4>* hessian_values, uint m)
{
  uint point = index_x(blockDim, blockIdx, threadIdx);

  scalar_type partial_density = 0.0f;
  vec_type<scalar_type,4> dxyz, dd1, dd2;
  if (!lda) { dxyz = dd1 = dd2 = vec_type<scalar_type,4>(0.0f,0.0f,0.0f,0.0f); }

  bool valid_thread = (point < points);
  scalar_type point_weight;
  if (valid_thread) point_weight = point_weights[point];

  __shared__ scalar_type rdm_sh[DENSITY_BATCH_SIZE];
  __shared__ scalar_type fj[DENSITY_BATCH_SIZE];
  __shared__ vec_type<scalar_type,4> fgj[DENSITY_BATCH_SIZE];
  __shared__ vec_type<scalar_type,4> fhj[DENSITY_BATCH_SIZE];
  __shared__ vec_type<scalar_type,4> fh2j[DENSITY_BATCH_SIZE];

  /***** compute density ******/
  for (uint i = 0; i < m; i++) {
    scalar_type w = 0.0f;
    vec_type<scalar_type,4> w3, ww1, ww2;
    if (!lda) { w3 = ww1 = ww2 = vec_type<scalar_type,4>(0.0f,0.0f,0.0f,0.0f); }

    scalar_type Fi;
    vec_type<scalar_type,4> Fgi, Fhi1, Fhi2;

    if (valid_thread) {
      Fi = function_values[COALESCED_DIMENSION(points) * i + point];
      if (!lda) {
        Fgi = gradient_values[COALESCED_DIMENSION(points) * i + point];
        Fhi1 = hessian_values[COALESCED_DIMENSION(points) * (2 * i + 0) + point];
        Fhi2 = hessian_values[COALESCED_DIMENSION(points) * (2 * i + 1) + point];
      }
    }

    for (uint bj = 0; bj <= i; bj += DENSITY_BATCH_SIZE) {
      __syncthreads();
      if (threadIdx.x < DENSITY_BATCH_SIZE) {
        if (bj + threadIdx.x <= i) rdm_sh[threadIdx.x] = rdm[COALESCED_DIMENSION(m) * i + (bj + threadIdx.x)]; // TODO: uncoalesced. invertir triangulo?
        else rdm_sh[threadIdx.x] = 0.0f;
       fj[threadIdx.x]=function_values[COALESCED_DIMENSION(points) * (bj+threadIdx.x) + point];
       fgj[threadIdx.x]=gradient_values[COALESCED_DIMENSION(points) * (bj+threadIdx.x) + point];
       fhj[threadIdx.x]=hessian_values[COALESCED_DIMENSION(points) * 2*(bj+threadIdx.x) + point];
       fh2j[threadIdx.x]=hessian_values[COALESCED_DIMENSION(points) * (2*(bj+threadIdx.x)+1) + point];

      }
      __syncthreads();

      if (valid_thread) {

//        uint COAL = COALESCED_DIMENSION(points);

        for (uint j = 0; j < DENSITY_BATCH_SIZE && (bj + j) <= i; j++) {

//           uint bjj = bj + j;

//          float Fj = function_values[COAL*bjj + point];
          w += rdm_sh[j] * fj[j];

          if (!lda) {
//            vec_type<scalar_type,4> Fgj = gradient_values[COAL * bjj + point];
            w3 += fgj[j] * rdm_sh[j];
//            vec_type<scalar_type,4> Fhj1 = hessian_values[COAL * (2 * bjj + 0) + point];
//            vec_type<scalar_type,4> Fhj2 = hessian_values[COAL * (2 * bjj + 1) + point];
            ww1 += fhj[j] * rdm_sh[j];
            ww2 += fh2j[j] * rdm_sh[j];
          }
        }
      }
    }

    partial_density += Fi * w;

    if (!lda) {
      dxyz += Fgi * w + w3 * Fi;
      dd1 += Fgi * w3 * 2.0f + Fhi1 * w + ww1 * Fi;

      vec_type<scalar_type,4> FgXXY(Fgi.x, Fgi.x, Fgi.y, 0.0f);
      vec_type<scalar_type,4> w3YZZ(w3.y, w3.z, w3.z, 0.0f);
      vec_type<scalar_type,4> FgiYZZ(Fgi.y, Fgi.z, Fgi.z, 0.0f);
      vec_type<scalar_type,4> w3XXY(w3.x, w3.x, w3.y, 0.0f);

      dd2 += FgXXY * w3YZZ + FgiYZZ * w3XXY + Fhi2 * w + ww2 * Fi;
    }
  }

  /***** compute energy / factor *****/
  scalar_type y2a, exc_corr;
  gpu_pot<scalar_type, compute_energy, true, lda>(partial_density, dxyz, dd1, dd2, exc_corr, y2a); // TODO: segundo parametro, que tenga en cuenta RMM+energy
  
  if (compute_energy && valid_thread)
    energy[point] = (partial_density * point_weight) * exc_corr;
  
  if (compute_factor && valid_thread)
    factor[point] = point_weight * y2a;
}
